# -*- coding: utf-8 -*-
"""t11c2-shadi-merge with confirmed cases and population.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XeMityBlecPV8kyatsoIy82Zml8qyr8Y
"""

# install.packages(c("data.table", "zoo"))# , "caret", "hexbin"))
!pip3 install --upgrade pandas

"""## Get git repo

In R notebook in colab, cannot mount g drive, so need to upload ssh key manually
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# mkdir -p $HOME/.ssh;
# cp '/content/drive/My Drive/id_rsa-gdrive_shadiakiki1986' $HOME/.ssh/id_rsa;
# #cp '/content/id_rsa-gdrive_shadiakiki1986' $HOME/.ssh/id_rsa;
# chmod 600 $HOME/.ssh/id_rsa;
# ###!cp '/content/drive/My Drive/id_rsa-gdrive_shadiakiki1986.pub' $HOME/.ssh/id_rsa.pub;
# ssh-keyscan -t rsa gitlab.com >> $HOME/.ssh/known_hosts;

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# rm -rf /content/covid19-testing;
# git clone --verbose --branch colab --depth 1 git@gitlab.com:biominers/covid19-testing.git /content/covid19-testing 2>&1;
# cd /content/covid19-testing && git status 2>&1;

"""## Load confirmed cases

Update, replaced reading our RData file with a raw kaggle fetch
"""

# Path to local data file
# rawdata_fn_filename = "covid19-global-forecasting-week-1.v20200323.raw.RData"
# rawdata_fn_filename = "covid19-global-forecasting-week-2.v20200330.raw.RData"
# rawdata_fn_filename = "covid19-global-forecasting-week-2.v20200331.raw.RData"
# rawdata_fn_filename = "covid19-global-forecasting-week-4.v20200408.raw.RData"
rawdata_fn_filename = "kaggle-confirmedCases.csv"
rawdata_fn_full = "/content/%s"%rawdata_fn_filename

# code copied from notebook t4e
from os.path import isfile

if not isfile(rawdata_fn_full):
  # download from kaggle directly
  if not isfile("/content/kaggle.json"):
    raise Exception("upload kaggle key")

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# 
# # Requires manually uploading the kaggle.json credentials file (locally in ~/shadi/.kaggle/kaggle.json)
# 
# ls /content/kaggle.json && \
#   pip3 install --user --upgrade kaggle==1.5.6 2>&1 && \
#   echo 'pip show kaggle' && \
#   pip3 show kaggle && \
#   mkdir -p /root/.kaggle/ && \
#   cp /content/kaggle.json /root/.kaggle/ && \
#   chmod 600 /root/.kaggle/kaggle.json && \
#   # kbin=/root/.local/bin/kaggle && \
#   kbin=/usr/local/bin/kaggle && \
#   echo 'kaggle version' && \
#   $kbin --version 2>&1 && \
#   echo 'kaggle download' && \
#   rm train.csv test.csv submission.csv && \
#   $kbin competitions download -c covid19-global-forecasting-week-4 2>&1

!cp train.csv covid19-testing/kaggle-confirmed.csv

if not isfile("/content/train.csv"):
  raise Exception("Failed to download")

# read csv files
import pandas as pd
conf_train=pd.read_csv("train.csv")

conf_train["Date"] = pd.to_datetime(conf_train["Date"])

conf_train["ConfirmedCases"] = conf_train.ConfirmedCases.astype(int)
conf_train["Fatalities"    ] = conf_train.Fatalities.astype(int)

conf_train.shape

#Combine country and province into a single string
def getCountryProv(r):
  if r.Province_State=="" or pd.isnull(r.Province_State): return r.Country_Region
  cp = "%s – %s"%(r.Country_Region, r.Province_State)
  return cp

conf_train["CountryProv"] = conf_train.apply(getCountryProv, axis=1)

conf_train.shape

conf_train.head()

# check dupes
if conf_train[["CountryProv","Date"]].duplicated().sum()>0:
  raise Exception("Found dupes")

"""## Load total tests

Manually upload the file ~~~`gdrive/biominers/shadi/multiple-aggregated_owid_wiki_worldometers-v20200406.csv`~~~

`gdrive/biominers/shadi/multiple-aggregated_owid_wiki_worldometers-gitrepo.csv` to this colab instance

(because R notebooks do not support mounting the google drive)
"""

dir_gitrepo = "/content/covid19-testing"

# totaltests_fn = "multiple-aggregated_owid_wiki_worldometers_biominers-v20200406.csv"
totaltests_fn = "multiple-aggregated_owid_wiki_worldometers_biominers-gitrepo.csv"
totaltests_fn = "%s/%s"%(dir_gitrepo, totaltests_fn)

if not isfile(totaltests_fn): raise Exception("Failed to find agg file")

totaltests_data = pd.read_csv(totaltests_fn)
totaltests_data["Date"] = pd.to_datetime(totaltests_data.Date)

# sort
totaltests_data.sort_values(["Location", "Date"], inplace=True)

# name corrections
totaltests_data.loc[ totaltests_data.Location=="Aruba",    "Location" ] = "Netherlands – Aruba"
totaltests_data.loc[ totaltests_data.Location=="Cayman Islands", "Location" ] = "United Kingdom – Cayman Islands"
totaltests_data.loc[ totaltests_data.Location=="Mayotte", "Location" ] = "France – Mayotte"
totaltests_data.loc[ totaltests_data.Location=="South Korea", "Location" ] = "Korea, South"
totaltests_data.loc[ totaltests_data.Location=="Taiwan", "Location" ] = "Taiwan*"
totaltests_data.loc[ totaltests_data.Location=="Bosnia", "Location" ] = "Bosnia and Herzegovina"
totaltests_data.loc[ totaltests_data.Location=="Bermuda", "Location" ] = "United Kingdom – Bermuda"
totaltests_data.loc[ totaltests_data.Location=="Brunei ", "Location" ] = "Brunei"
totaltests_data.loc[ totaltests_data.Location=="Greenland", "Location" ] = "Denmark – Greenland"

# drop dupes
totaltests_data = totaltests_data.loc[ ~totaltests_data[["Location","Date"]].duplicated(),]

totaltests_data.shape

# Need to drop dupes
# dim(totaltests_data) # 1485
# dim(unique(totaltests_data[,c("Location","Date")])) # 1431
if totaltests_data[["Location","Date"]].duplicated().sum()>0:
  raise Exception("Found dupes")
# totaltests_data[ duplicated(totaltests_data[,c("Location","Date")]), ]

totaltests_data.head()

len(totaltests_data.Location.unique())

"""## Merge together total tests with confirmed cases"""

# Down from 171 to 143 after fixing the separator to be long-dash
len(set(conf_train.CountryProv) - set(totaltests_data.Location))

set(conf_train.CountryProv) - set(totaltests_data.Location)

# "Aruba" %in% conf_train$CountryProv
# conf_train$CountryProv[grepl("Greenland", conf_train$CountryProv)]
totaltests_data.Location[totaltests_data.Location.str.contains("Kuwait")]

# conf_train.dtypes
totaltests_data.dtypes

conf_train = conf_train.merge(
                   totaltests_data,
                   left_on=["CountryProv","Date"],
                   right_on=["Location","Date"],
                   how='left', sort=False
                )
del conf_train['Location']
conf_train.shape

conf_train.head()

conf_train.set_index(["CountryProv","Date"], inplace=True)

# conf_train.loc["Lebanon"].tail()
conf_train.loc["Afghanistan"].head()

"""## Add population

Manually download

~~~`gdrive/Halim/Datasets/confirmed+population_v20200408.csv`~~~

~~~`gdrive/shadi/totaltests/t11c-population.csv`~~~

`gitlab.com/biominers/covid19-testing/t11c-country_metadata.csv`

and upload to colab instance
"""

#df_pop = read.csv("confirmed+population_v20200408.csv", stringsAsFactors=F)
#df_pop = unique(df_pop[,c("Country_Region", "Province_State","Population")])
#df_pop$CountryProv = with(df_pop,
#                          ifelse(Province_State=="",
#                                 Country_Region,
#                                 sprintf("%s – %s", Country_Region, Province_State)
#                                 ))
#dim(df_pop)

# save to csv
# write.csv(df_pop, "t11c-population.csv", row.names=F)

countrymeta_fn = "t11c-country_metadata.csv"
countrymeta_fn = "%s/%s" % (dir_gitrepo, countrymeta_fn)
df_pop = pd.read_csv(countrymeta_fn)

df_pop.head()

# unique(conf_train$CountryProv)
print("before merge")
print(conf_train.shape)

conf_train = conf_train.reset_index().merge(
                   df_pop[["CountryProv", "Population", "Lat", "Long"]],
                   on="CountryProv",
                   how='left', sort=False
                 )

print("after merge")
print(conf_train.shape)

# This should be enabled later
# conf_train["Lat_Long"] = conf_train["Lat"].map(str)+"_"+conf_train["Long"].map(str)

"""## Count per source of total tests"""

# entries from biominers
# sum(is.na(conf_train$total_cumul.all)) - sum(conf_train$total_cumul.source=="biominers", na.rm=T)
# sum(conf_train$total_cumul.source=="biominers", na.rm=T)

df_counts = conf_train.copy()
df_counts["total_cumul.source"] = df_counts["total_cumul.source"].apply(lambda x: "NA" if pd.isnull(x) else x)
df_counts = df_counts["total_cumul.source"].value_counts()
df_counts

# save
from os.path import join
df_counts.to_csv(join(dir_gitrepo, "count_sources.csv"), index=True)

conf_train.loc[conf_train["total_cumul.source"]=="biominers","CountryProv"].unique()
# unique(conf_train[conf_train$total_cumul.source=="wiki","CountryProv"])

conf_train.shape

"""## supplementary stat: tests per mil"""

import numpy as np
conf_train["tests_per_mil"] = conf_train.apply(lambda r: np.floor(r["total_cumul.all"]	/ r["Population"] * 1e6), axis=1)

# conf_train.reset_index(inplace=True)



# bring back the same order as earlier versions
colOrder = [
  "CountryProv", "Date",
  "Country_Region", "Province_State", "Id", "ConfirmedCases",
  "Fatalities", "Lat", "Long",
  #"Lat_Long", "is_validation", # lost after moving to using the kaggle dataset directly
  "total_cumul.all", "total_cumul.source", "Population", "tests_per_mil"
]

if len(set(conf_train.columns) - set(colOrder))>0:
  raise Exception("Extra columns")

if len(set(colOrder) - set(conf_train.columns))>0:
  raise Exception("Missing columns")

conf_train.columns

conf_train = conf_train[colOrder]
conf_train.columns

conf_train.shape

"""## Supplementary stats

- Tests per million
- positive/negative splits
- percentage confirmed of total
"""

conf_train.dtypes

# fix few outliers where total tests < confirmed cases and is negligible

# create a bi-index since it's not working in pandas
conf_train["UID"] = conf_train["CountryProv"]+"/"+conf_train.Date.dt.strftime("%Y-%m-%d")
conf_train.set_index("UID", inplace=True)

# overwrite (sources are worldometers.info and confirmed cases are not so far from total tests)
conf_train.loc["Burundi/2020-04-04","total_cumul.all"] = 3
conf_train.loc["Moldova/2020-04-04","total_cumul.all"] = 752
conf_train.loc["Senegal/2020-04-04","total_cumul.all"] = 219
conf_train.loc["Sierra Leone/2020-04-04","total_cumul.all"] = 4

#
conf_train.reset_index(inplace=True)

del conf_train["UID"]

# any outliers where total tests < confirmed cases ?
# conf_train[  conf_train["total_cumul.all"] < conf_train["ConfirmedCases"]  ].head(n=10)
if (conf_train["total_cumul.all"] < conf_train["ConfirmedCases"]).any():
  raise Exception("found tests<confirmed")

# Fix some outliers where total tests < confirmed cases
# Update: no need after fixing the above
# conf_train["total_cumul.all"]	= conf_train[["total_cumul.all","ConfirmedCases"]].max(axis=1)





conf_train["ratio_confirmed_total_pct"] = conf_train["ConfirmedCases"]/conf_train["total_cumul.all"]*100
conf_train["negative_cases"] = conf_train["total_cumul.all"] - conf_train["ConfirmedCases"]

# check missing Lat/Long
if pd.isnull(conf_train.Lat ).any(): raise Exception("Found some null Lats" )
if pd.isnull(conf_train.Long).any(): raise Exception("Found some null Longs")

# check Population
# summary(conf_train$Population)
conf_train.set_index(["CountryProv","Date"], inplace=True)
conf_train.loc["US – Illinois"].Population.unique()

conf_train.tests_per_mil.describe()

# visually check
# summary(conf_train$ratio_confirmed_total_pct)
# conf_train[conf_train$ratio_confirmed_total_pct > 100, "ratio_confirmed_total_pct"]
if not (conf_train.ratio_confirmed_total_pct.fillna(0) <= 100).all():
  raise Exception("Found some conf/total > 100")
# any(conf_train$ratio_confirmed_total_pct > 100, na.rm=T)

conf_train.negative_cases.describe()

# tail(conf_train)
conf_train.loc["Lebanon"].tail()

"""## save"""

# FIXME
!cd /content/covid19-testing/ && rm -rf count_sources.csv kaggle-confirmed.csv && git checkout * && git pull

# FIXME set precision to 14 digits after the decimal
# conf_train["ratio_confirmed_total_pct"] = conf_train["ratio_confirmed_total_pct"].round(13)
conf_train["ratio_confirmed_total_pct"] = conf_train["ratio_confirmed_total_pct"].round(2)

#conf_train["ratio_confirmed_total_pct"].loc["Algeria"].tail(n=10).round(14)

# FIXME enable this later, but keeping as double for now to support NA (instead of converting to something like 0's)
# Update: doesn't work for NA's
# conf_train["total_cumul.all"] = conf_train["total_cumul.all"].astype(int)

# save
#stop("uncomment to overwrite")

save_fn = "t11c-confirmed+totalTests-historical.csv"
save_fn = "%s/%s"%(dir_gitrepo, save_fn)
# Use na="" since arcgis.com doesn't support values for NAs
# write.csv(conf_train, "t11c-confirmed+totalTests-v20200406-historical.csv")
conf_train.reset_index().to_csv(save_fn, na_rep="", index=False) # , quote=False

save_fn

#!grep Korea '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'

# quote "Korea, South" in the csv file (and other country/states containing comas)
# In Python, no need for this
#system('sed --version', intern=T)
# system('sed -i "s/Korea, South/\\"Korea, South\\"/g" /content/covid19-testing/t11c-confirmed+totalTests-historical.csv 2>&1', intern=T)
# system('sed -i "s/Netherlands – Bonaire, Sint Eustatius and Saba/\\"Netherlands – Bonaire, Sint Eustatius and Saba\\"/g" /content/covid19-testing/t11c-confirmed+totalTests-historical.csv 2>&1', intern=T)
# system('sed -i "s/,Bonaire, Sint Eustatius and Saba/,\\"Bonaire, Sint Eustatius and Saba\\"/g" /content/covid19-testing/t11c-confirmed+totalTests-historical.csv 2>&1', intern=T)

"""Extract the `latestOnly` version manually by opening the historical in a worksheet editor and creating a pivot table.

## git commit
"""

save_fn

conf_train.columns

# Replace True by TRUE (difference between python and R)
# FIXME remove these differences later
#!sed -i s/True/TRUE/g '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'
#!sed -i s/False/FALSE/g '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'
#!sed -i s/\\.0,/,/g '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'
#!sed -i s/\\.0_/_/g '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'
#!sed -i s/\\.0$//g '/content/covid19-testing/t11c-confirmed+totalTests-historical.csv'

# !cd /content/covid19-testing && git diff|grep Zimbab|grep 2020-04-03|head
!cd /content/covid19-testing && git diff|grep Afghan|grep 2020-01-22|head

!cd /content/covid19-testing && git diff|head -n 20









# system("cd /content/covid19-testing && git status", intern=T)
system("cd /content/covid19-testing &&
        git config --global user.email 'shadiakiki1986@gmail.com' &&
        git config --global user.name 'Shadi Akiki'
        ", intern=T)

# system("cd /content/covid19-testing && git status", intern=T)
system("cd /content/covid19-testing &&
        git status &&
        git commit -am 'notebook t11c: updated t11c-confirmed+totalTests-historical.csv' 2>&1 &&
        git push &&
        echo 'done'
        ", intern=T)

